# 개발 일지 - 2026년 2월 7일

## 프로젝트: YOLO 안전모/사람/쓰러짐 탐지 모델 (Hoban)
- **모델**: yolo26m.pt (YOLO v26 Medium, 42MB)
- **4 클래스**: helmet_o(0), helmet_x(1), person(2), fallen(3)
- **작업 시간**: 01:07 ~ 12:25 (약 11시간)
- **세션 수**: 4개 (컨텍스트 초과로 세션 전환 3회)

---

## 01:00~02:00 | Person 데이터가 문제다

### 문제 인식
기존 datasets에서 학습한 모델의 person, fallen 클래스 성능이 유독 낮았다. mAP 0.9를 목표로 잡았지만 person/fallen bbox 정확도가 기대에 미치지 못했다.

### 원인 탐색
`analyze_data.py`로 데이터를 뜯어보니 aihub에서 가져온 person 데이터의 품질이 의심됐다. 전체 데이터를 무작정 넣는 것보다 최소한의 고품질 데이터로 실험하는 방향을 잡았다.

v5 서브셋으로 실험했지만 person/fallen이 여전히 안 나왔고, fastdup으로 중복/저품질을 제거해봐도 근본적 해결이 안 됐다.

### 결정: WiderPerson으로 교체 (v6)
- **이유**: 기존 aihub person 데이터 자체가 탐지 학습에 부적합하다고 판단
- WiderPerson (cbsr.ia.ac.cn)은 보행자 탐지 전용 데이터셋으로, bbox가 전신을 정확히 잡고 있어 YOLO 학습에 더 적합
- 기존 person 데이터를 전량 제거하고 WiderPerson으로 v6 구성

---

## 02:00~03:00 | WiderPerson vs COCO, 어떤 Person이 더 나은가

### 실험 설계
- **로컬(RTX 5070 Ti)**: 서브셋으로 빠른 실험, **서버**: 풀 데이터 학습
- 이 분리로 빠른 반복 실험이 가능해짐

### 새로운 후보: COCO Person
WiderPerson으로 v6를 돌리는 동안, COCO 2017의 person 데이터(3만장)도 병렬로 다운로드했다. 두 소스를 비교하기 위함이었다.

### v6 결과 → v7 결정
v6(WiderPerson) 학습 결과를 확인한 뒤, 동일 조건에서 COCO person으로 교체한 v7을 실험했다.

| 지표 | v6 (WiderPerson) | v7 (COCO) |
|------|------------------|-----------|
| Background FP율 | 58% | **47%** |

- **결정: COCO person 채택 (v7)**
- **이유**: COCO가 FP(오탐)율이 11%p나 낮았다. WiderPerson은 보행자 특화지만 다양한 배경/환경이 부족해 background FP가 많이 발생. COCO는 일상 장면이 다양해 모델이 "사람이 아닌 것"도 잘 구분함.

---

## 04:00~05:00 | 클래스 불균형 문제와 bbox 기반 샘플링

### 문제: helmet_x 데이터 부족
v7 풀 빌드 시 클래스당 10,000장을 목표로 했는데, helmet_x(미착용)가 **4,841 bbox밖에 없었다**. 다른 클래스 대비 절반 수준.

### 결정: 이미지 수가 아닌 bbox 수 기반 균형
- **이유**: 이미지 1장에 bbox가 여러 개일 수 있으므로, 이미지 수 기준 균형은 실제 학습 데이터 양과 괴리가 생긴다. bbox 수 기준으로 맞추면 모델이 각 클래스를 균등하게 학습.
- `select_by_bbox()` 함수 도입: 셔플 후 목표 bbox 수에 도달할 때까지 이미지를 선택하는 그리디 방식
- helmet_x는 4,841 전량 사용, 나머지는 10,000 bbox까지 선택

### fastdup 환경 구축
- **문제**: fastdup이 Windows를 지원하지 않음
- **해결**: WSL Ubuntu에 Python 3.10 전용 환경 구축 (`~/fastdup_env/bin/python`)
- v7 전체 데이터에 대해 fastdup 중복 분석 실행 → 유사도 > 0.9 기준 중복 제거

---

## 06:00~07:00 | Fallen 데이터의 신뢰성 문제

### 문제: fallen 데이터가 단일 소스
v7까지 fallen(쓰러짐)은 roboflow 단일 소스에서만 가져왔다. 단일 소스의 편향이 모델 일반화에 악영향을 줄 수 있었다.

### 탐색: 8개 fall 소스 발견
`D:\task\dataset`에 8개 fall 관련 폴더가 있었다. 분석 결과:

| 소스 | 이미지 | fallen bbox | 특이사항 |
|------|--------|-------------|----------|
| fall detection ip camera | 6,745 | 4,285 | IP 카메라 영상 |
| Fall Detection (aug3x) | 9,438 | 9,441 | **3배 증강 데이터** |
| Fall.v1i | 5,445 | 6,212 | down 클래스만 사용 |
| fall.v1i (2) | 5,072 | 6,364 | |
| fall.v2i | 1,659 | 1,016 | |
| Fall.v3i | 5,313 | 5,460 | |
| fall.v4i | 7,775 | 2,309 | |
| fallen.v2i | 14,607 | 30,971 | 최대 소스 |

총 ~96,000+ fallen bbox. 하지만 **품질을 확신할 수 없었다**.

### 결정: 전량 수집 → fastdup 정제 (v8)
- **이유**: 개별 소스를 눈으로 검증하는 건 불가능. 대신 모두 모은 뒤 fastdup으로 기계적으로 정제하는 것이 효율적이고 신뢰성 있음.
- fire_smoke 폴더와 datasets_clean은 제외 (사용자 지시)
- helmet은 helmet_30k(aihub JSON→YOLO 변환), person은 coco_person 유지

### 실행 결과
- 8개 소스 통합 → fallen_pool (56,054장)
- fastdup 분석 → **82,714 중복 쌍 발견 (유사도 > 0.9), 55% 중복!**
- 중복 제거 후 → 23,661장 / 33,018 bbox
- **교훈**: roboflow 출처 데이터는 소스 간 교차 중복이 매우 심함. fastdup 필수.

---

## 08:00~09:00 | "데이터 양을 올린다고 답은 아니잖아"

### v8 서브셋 실험 결과
v8(multi-source fallen + coco person + helmet_30k)을 3K bbox/cls, 30에폭으로 로컬 실험:
- **mAP50=0.801, mAP50-95=0.491**

confusion matrix에서 **3대 문제** 발견:

| 문제 | 수치 | 심각도 |
|------|------|--------|
| Person miss rate | **38%** (230/602 놓침) | 높음 |
| Background FP | **605건** | 높음 |
| Helmet o↔x 혼동 | **109건** | 중간 |

### 잘못된 접근 시도
처음에는 "데이터 양을 늘리면 해결될 것"이라고 제안했다.

**사용자 피드백**: *"데이터 양을 무조건 올린다고 답은 아니잖아"*

이 피드백이 오늘의 가장 중요한 전환점이었다.

### 근본 원인 분석

**1. Person miss 38% → COCO bbox가 너무 작았다**
- `check_coco_quality.py`로 분석: COCO person bbox의 **48%가 area < 1%**
- 640px 이미지에서 area 1% = 약 64px. 이 크기는 YOLO가 탐지하기 거의 불가능
- 모델이 "작은 사람도 학습"하려다 보니 정상 크기 사람도 놓치는 부작용

**2. Background FP 605건 → 네거티브 샘플 부재**
- 학습 데이터에 "객체가 없는 이미지"가 전혀 없었다
- 모델이 "무언가는 반드시 있다"고 학습 → 빈 배경에서도 오탐 발생

**3. Helmet o↔x 혼동 109건 → 헬멧 bbox가 HEAD-ONLY**
- `visualize_bbox.py`로 시각화해보니 **헬멧 bbox가 머리만 잡고 있었다** (전신 아님)
- helmet_o: **69.2%**가 area < 0.5% (640px에서 ~45px)
- helmet_x: **81.9%**가 area < 0.5%
- 너무 작은 bbox에서 "쓴 것 vs 안 쓴 것"을 구분하는 건 모델에게 무리한 요구

### 핵심 교훈
> **데이터 품질 > 데이터 양**. 탐지 불가능한 작은 bbox를 아무리 많이 넣어봐야 모델을 혼란시킬 뿐이다. 학습 가능한 크기의 bbox만 남기는 것이 핵심.

---

## 09:00~10:00 | 품질 필터링 전략 수립 (v9)

### 결정: area 기반 필터링
각 데이터 소스별로 "탐지 가능한 최소 크기" 기준을 설정:

| 데이터 | 필터 기준 | 이유 |
|--------|-----------|------|
| COCO person | area >= **2%** | 640px에서 ~90px. 안정적 탐지 가능 |
| Fallen pool | **0.5%** <= area <= **70%** | 극단적 크기만 제거, 쓰러진 자세는 넓적 |
| Helmet (사용자 제공) | area >= **2%** (착용) / **1%** (미착용) | 미착용은 데이터 적어 기준 완화 |
| Negative samples | 빈 라벨 이미지 **1K장** 추가 | FP 감소 목적 |

### 필터링 결과

| 데이터 | 필터 전 | 필터 후 | 제거율 |
|--------|---------|---------|--------|
| COCO person | 29,778장 / 195,377 bbox | 23,100장 / 76,453 bbox | **61% bbox 제거** |
| Fallen pool | 23,661장 / 33,018 bbox | 21,487장 / 27,105 bbox | 18% bbox 제거 |

COCO에서 bbox의 61%가 제거됐다는 건, 기존에 탐지 불가능한 bbox가 얼마나 많았는지를 보여준다.

### 헬멧 데이터: aihub JSON → YOLO 변환
- 사용자가 `helmet_21k` (area 기준 상위 추출) 제공
- **문제**: aihub 라벨이 JSON 형식 (YOLO txt가 아님)
- `convert_helmet_21k.py`로 변환. 상위 0.1% 이상치 확인 → 클로즈업 샷으로 허용 가능
- 결과: 20K장, cls0: 10,845 bbox / cls1: 11,500 bbox

---

## 10:00~11:00 | v9 실험 — 품질 필터링의 효과 입증

### v9 서브셋 실험 (3K bbox/cls, 30에폭)

| 지표 | v8_sub (필터 전) | v9_sub (필터 후) | 변화 |
|------|-----------------|-----------------|------|
| mAP50 | 0.801 | **0.868** | **+6.7%p** |
| mAP50-95 | 0.491 | **0.593** | **+10.2%p** |
| Precision | 0.768 | **0.814** | +4.6%p |
| Recall | 0.820 | **0.823** | +0.3%p |

**같은 양(3K bbox/cls), 같은 에폭(30)인데 필터링만으로 mAP50-95가 10%p 상승.**

이것이 "데이터 양 < 데이터 품질"을 숫자로 증명한 결과다.

### 디스크 정리
실험이 검증되었으므로 불필요 데이터를 정리:
- ~72GB 삭제 (helmet_pool, datasets_v8, datasets_clean, WiderPerson 등)
- 필요 데이터만 `dataset/` 하위로 재구성
- 최종 용량: 21.55GB

---

## 12:00~12:30 | v9 풀 스케일업 (서버 학습 준비)

### 결정: 30K bbox/cls로 확대
v9 서브셋(3K)에서 품질 필터링의 효과가 입증됐으므로, 이제 양을 늘려 성능을 극대화할 차례.

- 사용자가 `helmet_60k` 제공 (aihub에서 area 기준 상위 30K씩 추출, 이상치 >=20% 제거 완료)
- JSON→YOLO 변환 후 60,000장 / cls0: 50,250 bbox / cls1: 79,332 bbox

### 최종 datasets_v9 (풀)

| 클래스 | bbox | 소스 | 품질 보증 |
|--------|------|------|----------|
| helmet_o (0) | **30,000** | helmet_60k (area 3.5~20%) | 사용자 area 필터링 |
| helmet_x (1) | **30,000** | helmet_60k (area 0.4~20%) | 사용자 area 필터링 |
| person (2) | **30,000** | COCO 2017 (area >= 2%) | 필터링 스크립트 |
| fallen (3) | **27,105** | 8소스 통합 (0.5~70%) | fastdup + area 필터링 |
| negative | **1,000장** | aihub background | 빈 라벨 |
| **합계** | **117,105 bbox** | **60,798장** | |

- fallen이 30K에 못 미치는 건 가용 데이터 한계 (27K가 전량)

### 서버 학습 설정
- **epochs: 300** (patience=30으로 early stop 가능)
- batch=24, workers=8, AdamW, cos_lr
- v9_sub와 동일 augmentation/loss 설정 유지

---

## 오늘의 핵심 의사결정 흐름

```
Person 성능 저하 발견
  → aihub person 의심 → WiderPerson 교체 (v6)
  → COCO와 비교 실험 → COCO가 FP 낮음 → COCO 채택 (v7)

Fallen 단일 소스 편향 우려
  → 8개 소스 전량 수집 → fastdup으로 55% 중복 제거 (v8)

v8 실험: mAP 정체
  → "데이터 양 ≠ 답" 피드백
  → 근본 원인 분석: 48% bbox가 탐지 불가 크기
  → area 기반 필터링 도입 (v9)
  → 같은 양인데 mAP50-95 +10.2%p 상승

품질 필터링 효과 입증
  → 30K bbox/cls로 스케일업 → 서버 학습 준비 완료
```

### 기술적 교훈 정리

| 교훈 | 근거 |
|------|------|
| 데이터 품질 > 데이터 양 | 필터링만으로 mAP50-95 +10.2%p |
| bbox area 필터링 필수 | COCO의 48%가 탐지 불가 크기였음 |
| Negative samples 필요 | 빈 배경 이미지 없으면 FP 급증 |
| 다중 소스 = 중복 주의 | fall 8개 소스 55%가 중복 |
| 서브셋 실험 → 풀 스케일업 | 3K로 검증 후 30K로 확장하는 단계적 접근 |
| Head-only bbox는 크기가 핵심 | 헬멧 bbox 82%가 area < 0.5% |

### 다음 단계
- [ ] 서버에 datasets_v9 + train_v9_server.py 배포
- [ ] 서버 학습 실행 (300에폭, batch=24, patience=30)
- [ ] v9 풀 학습 결과 확인 및 v8_sub/v9_sub와 비교 분석
