# go2k v2 학습 계획: v13 서브샘플 + go2k 오버샘플링

## 목표

v13 범용 데이터의 지식을 유지하면서 CCTV 도메인(go2k)에 특화된 모델 학습

## 현재 데이터

| 데이터셋 | 이미지 | helmet_o (cls0) | helmet_x (cls1) | 비고 |
|----------|--------|-----------------|-----------------|------|
| v13 train | 36,000장 | 24,049 | 26,335 | 범용 (다양한 출처) |
| v13 valid | 8,999장 | - | - | |
| go2k_manual | 599장 | 1,449 | 231 | CCTV 수동 라벨링 |

## 데이터 구성 계획

### Train

| 소스 | 샘플링 | 이미지 수 | 비중 |
|------|--------|----------|------|
| v13 train에서 랜덤 샘플 | 8,000장 | 8,000 | 63% |
| go2k_manual × 8 오버샘플 | 599 × 8 = 4,792 | 4,792 | 37% |
| **합계** | | **12,792** | |

- v13 8K: 범용 지식 유지 (원본 36K의 22%)
- go2k × 8: CCTV 도메인 비중 37% → 충분한 영향력
- 오버샘플링 과적합은 데이터 증강(mosaic, mixup, hsv, flip 등)으로 완화

### Valid

| 소스 | 이미지 수 | 비고 |
|------|----------|------|
| v13 valid에서 랜덤 샘플 | 1,000장 | 범용 성능 확인 |
| go2k_manual에서 분리 (20%) | 120장 | CCTV 성능 확인 |
| **합계** | **1,120** | |

- go2k는 train/valid 분리 (80/20) → valid에 포함된 go2k로 CCTV 도메인 성능 확인 가능

## 학습 설정

| 항목 | 값 | 비고 |
|------|-----|------|
| 베이스 모델 | v13 stage2 best.pt | go500이 아닌 원본에서 시작 |
| optimizer | SGD | 안정성 (AdamW 붕괴 방지) |
| lr0 | 0.005 | fine-tune 적정 |
| imgsz | 640 | |
| epochs | 100 | patience=20 |
| batch | 16 | |

## 구현 순서

1. go2k_manual을 train/valid 분리 (80/20, seed=42)
2. v13 train에서 8,000장 랜덤 샘플
3. v13 valid에서 1,000장 랜덤 샘플
4. go2k train분을 8배 복제 (심볼릭 링크 or 파일 복사)
5. 합친 데이터셋 `datasets_go2k_v2/` 생성
6. data.yaml 작성
7. 학습 스크립트 `train_go2k_v2.py` 작성
8. 학습 실행
